{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ICHI Ablation best.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqqBi37-1Oo5",
        "outputId": "01f45bf1-8b6b-44ba-9a34-1fc39664c043"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed May 12 10:32:47 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK9TuU8wFkVa"
      },
      "source": [
        "#%cd ..\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dK-PDhIFqc8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwhsAMZ0F_ZZ"
      },
      "source": [
        "# !cp -a ichi-datasets/. transformers/\n",
        "# %cd transformers/\n",
        "# !mkdir examples/cadec/\n",
        "# !mkdir examples/ichi/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZaTPlSZGbfh",
        "outputId": "27e1944b-e055-4233-ea1f-3171f064f692"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqx9cKKKkqBr",
        "outputId": "1ea19303-feaa-4c1c-d497-257c693ba6fb"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers/\n",
        "!git checkout 896a0eb1fd861bc37097a9b669ebf4cb8d523de7\n",
        "%cd ..\n",
        "!git clone https://github.com/frozen-walker/ichi_datasets.git\n",
        "!cp -a ichi_datasets/. transformers/\n",
        "%cd transformers/\n",
        "!mkdir examples/cadec/\n",
        "!mkdir examples/ichi/\n",
        "!pip install -r requirements.txt\n",
        "!pip install .\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 72376, done.\u001b[K\n",
            "remote: Counting objects: 100% (421/421), done.\u001b[K\n",
            "remote: Compressing objects: 100% (260/260), done.\u001b[K\n",
            "remote: Total 72376 (delta 203), reused 276 (delta 136), pack-reused 71955\u001b[K\n",
            "Receiving objects: 100% (72376/72376), 55.20 MiB | 15.75 MiB/s, done.\n",
            "Resolving deltas: 100% (51334/51334), done.\n",
            "/content/transformers\n",
            "Note: checking out '896a0eb1fd861bc37097a9b669ebf4cb8d523de7'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 896a0eb1f Merge pull request #2459 from Perseus14/patch-4\n",
            "/content\n",
            "Cloning into 'ichi_datasets'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 26 (delta 8), reused 9 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (26/26), done.\n",
            "/content/transformers\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n",
            "Processing /content/transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (1.19.5)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/cb/3e8902d528538972873d0e9e4e47a31d1849a98e057009e9d383637c96fb/tokenizers-0.0.11-cp37-cp37m-manylinux1_x86_64.whl (4.7MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7MB 10.3MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/2f/8cfc267a6bc33a50ea899462ca5d85bf583acb0880cb91e1e533a6271bd3/boto3-1.17.71-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 48.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 47.9MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.4MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.71\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/7a/bcc46535f978318168fbc5d008c58141280939fea33dae12644b97ced403/botocore-1.20.71-py2.py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 40.4MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.71->boto3->transformers==2.3.0) (2.8.1)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.3.0-cp37-none-any.whl size=458674 sha256=01679ae1da18e57987a6cd835c62596c1f1ac7a0bd6ec2124b26fac880411cbf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iv9a5u6g/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "\u001b[31mERROR: botocore 1.20.71 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, jmespath, botocore, s3transfer, boto3, sentencepiece, sacremoses, transformers\n",
            "Successfully installed boto3-1.17.71 botocore-1.20.71 jmespath-0.10.0 s3transfer-0.4.2 sacremoses-0.0.45 sentencepiece-0.1.95 tokenizers-0.0.11 transformers-2.3.0\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QHvNWCi1Tcl"
      },
      "source": [
        "#!mkdir examples/ichi/    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8LgWLJLFQ5K",
        "outputId": "d8e7cc0a-ded8-4e03-ea5d-1daa50121287"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjU93qRLHSyu"
      },
      "source": [
        "#%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY-Q9UGlHYwC"
      },
      "source": [
        "#!cp -a ichi_datasets/. transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt4BfnCVGekl"
      },
      "source": [
        "#!mkdir newdata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC79BLFAGxvk"
      },
      "source": [
        "#!mkdir examples/ichi/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW5Ul74hB9yR"
      },
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"/content/ichi_datasets/ichi/final_train_result.tsv\",sep=\"\\t\")\n",
        "percentage = 0.5\n",
        "Classes = df_train.Category.unique()\n",
        "Classes\n",
        "df_final = pd.DataFrame(columns = ['Category','Title','Question','Concepts'])\n",
        "for i,cat in enumerate(Classes):\n",
        "  if i==0:\n",
        "    df_final = df_train[df_train[\"Category\"] == cat]\n",
        "    df_final = df_final.sample(frac=percentage)\n",
        "  else:\n",
        "    df_temp = df_train[df_train[\"Category\"] == cat]\n",
        "\n",
        "    # here random state is use as seed. See here: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html\n",
        "    df_temp = df_temp.sample(frac=percentage, random_state=50)\n",
        "    frames = [df_final,df_temp]\n",
        "    df_final = pd.concat(frames)\n",
        "df_final = df_final.sample(frac=1)\n",
        "df_final.to_csv('/content/transformers/ichi/temp_train.tsv',sep=\"\\t\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "-XXLwZi5T8uU",
        "outputId": "4a7833e3-a714-41be-dc02-6c268e043d6e"
      },
      "source": [
        "\n",
        "\n",
        "df_final_tmp_11112=pd.read_csv(\"/content/transformers/ichi/temp_train.tsv\",sep=\"\\t\")\n",
        "df_final_tmp_11112"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Title</th>\n",
              "      <th>Question</th>\n",
              "      <th>Concepts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRMT</td>\n",
              "      <td>Menoause after Hystorectomy</td>\n",
              "      <td>I am a 59 year old fitness instructor. I stopp...</td>\n",
              "      <td>symptoms|old|ovarian cancer|ovaries|age</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DEMO</td>\n",
              "      <td>Previous Ectopic now worried</td>\n",
              "      <td>I previously had an Ectopic pregnancy. The Dr ...</td>\n",
              "      <td>ovulation|confused|Ectopic|said|pregnancy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PREG</td>\n",
              "      <td>period after miscarriage</td>\n",
              "      <td>i had a miscarriage back in june 2006 ever sin...</td>\n",
              "      <td>regular period|miscarriage|ever|back</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DISE</td>\n",
              "      <td>HI ! Brake fullblown down</td>\n",
              "      <td>Hi been with group But no thing to report . No...</td>\n",
              "      <td>hallucination|Bill</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DISE</td>\n",
              "      <td>Hyperactive Airway Disease</td>\n",
              "      <td>My daughter was just discharged from the Air F...</td>\n",
              "      <td>asthma|Hyperactive|Hyperactive Airway Disease|...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3994</th>\n",
              "      <td>FAML</td>\n",
              "      <td>School Behavior Problems</td>\n",
              "      <td>What is normally the problem when a child who ...</td>\n",
              "      <td>mood swings,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3995</th>\n",
              "      <td>DISE</td>\n",
              "      <td>H1N1 leftover..?</td>\n",
              "      <td>hi! 48 yr old female...high blood pressure and...</td>\n",
              "      <td>procedure|palpitations|a burning|old|any disco...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3996</th>\n",
              "      <td>PREG</td>\n",
              "      <td>AF is expected tomorrow night... should I test...</td>\n",
              "      <td>Hello all. I am 11 DPO today and tomorrow I wi...</td>\n",
              "      <td>symptoms|nervous|pregnancy|all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3997</th>\n",
              "      <td>GOAL</td>\n",
              "      <td>staph marginal keratitis</td>\n",
              "      <td>Went to opthamoligist for itchy red eye.    di...</td>\n",
              "      <td>cornea|eye|diagnosed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3998</th>\n",
              "      <td>GOAL</td>\n",
              "      <td>Retinal Detachment surgery and eye is smaller ...</td>\n",
              "      <td>Hi, i had a retinal detachment approximately 4...</td>\n",
              "      <td>scleral buckle|eyes|retinal detachments|light|...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3999 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Category  ...                                           Concepts\n",
              "0        TRMT  ...            symptoms|old|ovarian cancer|ovaries|age\n",
              "1        DEMO  ...          ovulation|confused|Ectopic|said|pregnancy\n",
              "2        PREG  ...               regular period|miscarriage|ever|back\n",
              "3        DISE  ...                                 hallucination|Bill\n",
              "4        DISE  ...  asthma|Hyperactive|Hyperactive Airway Disease|...\n",
              "...       ...  ...                                                ...\n",
              "3994     FAML  ...                                       mood swings,\n",
              "3995     DISE  ...  procedure|palpitations|a burning|old|any disco...\n",
              "3996     PREG  ...                     symptoms|nervous|pregnancy|all\n",
              "3997     GOAL  ...                               cornea|eye|diagnosed\n",
              "3998     GOAL  ...  scleral buckle|eyes|retinal detachments|light|...\n",
              "\n",
              "[3999 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D79M5NpS8vh"
      },
      "source": [
        "# ########### Uncomment it to download Glove Vectors\n",
        "\n",
        "#!curl -O -J -L http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "#!unzip glove.840B.300d.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdLQyv7_-bPz",
        "outputId": "e8e5d8c1-732b-4679-837b-b9b8449a41ad"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed May 12 10:35:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-EzqBB_PvmT"
      },
      "source": [
        "# ########### Uncomment it to download BioASQ Embeddings\n",
        "\n",
        "# !curl -O -J -L http://bioasq.lip6.fr/tools/BioASQword2vec/\n",
        "# !tar -xvzf biomedicalWordVectors.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKzJUfduLaSk"
      },
      "source": [
        "#%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL0KtyeJkv6Z"
      },
      "source": [
        "# class ICHIDataset(Dataset):\n",
        "#     def __init__(self, fname, tokenizer):\n",
        "#         fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "#         lines = fin.readlines()\n",
        "#         fin.close()\n",
        "\n",
        "#         all_data = []\n",
        "#         for i in range(0, len(lines), 3):\n",
        "#             text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
        "#             aspect = lines[i + 1].lower().strip()\n",
        "#             polarity = lines[i + 2].strip()\n",
        "\n",
        "#             text_raw_indices = tokenizer.text_to_sequence(text_left + \" \" + aspect + \" \" + text_right)\n",
        "#             text_raw_without_aspect_indices = tokenizer.text_to_sequence(text_left + \" \" + text_right)\n",
        "#             text_left_indices = tokenizer.text_to_sequence(text_left)\n",
        "#             text_left_with_aspect_indices = tokenizer.text_to_sequence(text_left + \" \" + aspect)\n",
        "#             text_right_indices = tokenizer.text_to_sequence(text_right, reverse=True)\n",
        "#             text_right_with_aspect_indices = tokenizer.text_to_sequence(\" \" + aspect + \" \" + text_right, reverse=True)\n",
        "#             aspect_indices = tokenizer.text_to_sequence(aspect)\n",
        "#             left_context_len = np.sum(text_left_indices != 0)\n",
        "#             aspect_len = np.sum(aspect_indices != 0)\n",
        "#             aspect_in_text = torch.tensor([left_context_len.item(), (left_context_len + aspect_len - 1).item()])\n",
        "#             polarity = int(polarity) + 1\n",
        "\n",
        "#             text_bert_indices = tokenizer.text_to_sequence('[CLS] ' + text_left + \" \" + aspect + \" \" + text_right + ' [SEP] ' + aspect + \" [SEP]\")\n",
        "#             bert_segments_ids = np.asarray([0] * (np.sum(text_raw_indices != 0) + 2) + [1] * (aspect_len + 1))\n",
        "#             bert_segments_ids = pad_and_truncate(bert_segments_ids, tokenizer.max_seq_len)\n",
        "\n",
        "#             text_raw_bert_indices = tokenizer.text_to_sequence(\"[CLS] \" + text_left + \" \" + aspect + \" \" + text_right + \" [SEP]\")\n",
        "#             aspect_bert_indices = tokenizer.text_to_sequence(\"[CLS] \" + aspect + \" [SEP]\")\n",
        "\n",
        "#             data = {\n",
        "#                 'text_bert_indices': text_bert_indices,\n",
        "#                 'bert_segments_ids': bert_segments_ids,\n",
        "#                 'text_raw_bert_indices': text_raw_bert_indices,\n",
        "#                 'aspect_bert_indices': aspect_bert_indices,\n",
        "#                 'text_raw_indices': text_raw_indices,\n",
        "#                 'text_raw_without_aspect_indices': text_raw_without_aspect_indices,\n",
        "#                 'text_left_indices': text_left_indices,\n",
        "#                 'text_left_with_aspect_indices': text_left_with_aspect_indices,\n",
        "#                 'text_right_indices': text_right_indices,\n",
        "#                 'text_right_with_aspect_indices': text_right_with_aspect_indices,\n",
        "#                 'aspect_indices': aspect_indices,\n",
        "#                 'aspect_in_text': aspect_in_text,\n",
        "#                 'polarity': polarity,\n",
        "#             }\n",
        "\n",
        "#             all_data.append(data)\n",
        "#         self.data = all_data\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         return self.data[index]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG8YjUsTm2Eg",
        "outputId": "bc5a3cc1-2fe7-4922-f0e3-a813a367fa42"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGkNpSjDrXjh",
        "outputId": "f731e296-3587-42fc-a52d-cdf62e835c53"
      },
      "source": [
        "%%writefile /content/transformers/examples/ichi/utils_ichi.py\n",
        "import logging\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "import csv\n",
        "import pickle\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from sklearn.metrics import matthews_corrcoef, f1_score\n",
        "from transformers.file_utils import is_tf_available \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "if is_tf_available():\n",
        "    import tensorflow as tf\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "def acc_and_f1(preds, labels):\n",
        "    acc = simple_accuracy(preds, labels)\n",
        "    f1 = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
        "    return {\n",
        "        \"acc\": acc,\n",
        "        \"f1\": f1,\n",
        "    }\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    # return {\"acc\": simple_accuracy(preds, labels)}\n",
        "    return acc_and_f1(preds, labels)\n",
        "\n",
        "\n",
        "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='pre', truncating='post', value=0):\n",
        "    x = (np.ones(maxlen) * value).astype(dtype)\n",
        "    if truncating == 'pre':\n",
        "        trunc = sequence[-maxlen:]\n",
        "    else:\n",
        "        trunc = sequence[:maxlen]\n",
        "    trunc = np.asarray(trunc, dtype=dtype)\n",
        "    if padding == 'post':\n",
        "        x[:len(trunc)] = trunc\n",
        "    else:\n",
        "        x[-len(trunc):] = trunc\n",
        "    return x\n",
        "\n",
        "class Tokenizer(object):\n",
        "    def __init__(self, max_seq_len, max_num_words=None,lower=True):\n",
        "        self.lower = lower\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.word2idx = {}\n",
        "        self.word_freq = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx2word[0] = '<PAD>'\n",
        "        self.word2idx['<PAD>'] = 0\n",
        "        self.word_freq['<PAD>'] = 100000\n",
        "        self.idx = 1\n",
        "        self.max_num_words = max_num_words\n",
        "\n",
        "    def fit_on_text(self, text):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = self.idx\n",
        "                self.word_freq[word] = 1\n",
        "                self.idx2word[self.idx] = word\n",
        "                self.idx += 1\n",
        "            else:\n",
        "                self.word_freq[word] = self.word_freq[word] + 1\n",
        "    \n",
        "    def update_tokenizer(self):\n",
        "        if self.max_num_words == None:\n",
        "            return\n",
        "        elif self.max_num_words >= self.idx:\n",
        "            return \n",
        "        else:\n",
        "            del self.word_freq['<PAD>']\n",
        "            self.word_freq = {k: v for k, v in sorted(self.word_freq.items(), key=lambda item: item[1], reverse=True)}\n",
        "            self.word2idx = {}\n",
        "            self.idx2word = {}\n",
        "            self.idx2word[0] = '<PAD>'\n",
        "            self.word2idx['<PAD>'] = 0\n",
        "            self.idx = 1\n",
        "            for i, key in enumerate(self.word_freq):\n",
        "                if i >= self.max_num_words:\n",
        "                    break\n",
        "                else:\n",
        "                    self.word2idx[key] = i+1\n",
        "                    self.idx2word[i+1] = key\n",
        "                    self.idx += 1\n",
        "            self.word_freq['<PAD>'] = 100000\n",
        "\n",
        "\n",
        "\n",
        "    def fit_on_examples(self, examples):\n",
        "        is_tf_dataset = False\n",
        "        if is_tf_available() and isinstance(examples, tf.data.Dataset):\n",
        "            is_tf_dataset = True\n",
        "        processor = ICHIProcessor()\n",
        "        for example in examples:\n",
        "            if is_tf_dataset:\n",
        "                example = processor.get_example_from_tensor_dict(example)\n",
        "                example = processor.tfds_map(example)\n",
        "            self.fit_on_text(example.clean_text)\n",
        "\n",
        "    def text_to_sequence(self, text, reverse=False, padding='pre', truncating='post'):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        unknownidx = len(self.word2idx)+1\n",
        "        if (self.max_num_words == None) or (self.max_num_words >= self.idx):\n",
        "            sequence = [self.word2idx[w] if w in self.word2idx else unknownidx for w in words]\n",
        "        else:\n",
        "            sequence = []\n",
        "            for w in words:\n",
        "                if w in self.word2idx:\n",
        "                    if self.word2idx[w] > self.max_num_words:\n",
        "                        sequence.append(unknownidx)\n",
        "                    else:\n",
        "                        sequence.append(self.word2idx[w])\n",
        "                else:\n",
        "                    sequence.append(unknownidx)\n",
        "        if len(sequence) == 0:\n",
        "            sequence = [0]\n",
        "        if reverse:\n",
        "            sequence = sequence[::-1]\n",
        "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
        "\n",
        "def _load_word_vec_glove(path, word2idx=None):\n",
        "    fin = open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    word_vec = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split()\n",
        "        if word2idx is None or tokens[0] in word2idx.keys():\n",
        "            try:\n",
        "                word_vec[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n",
        "            except:\n",
        "                pass\n",
        "    return word_vec\n",
        "    \n",
        "def build_embedding_matrix_glove(word2idx, embed_dim, dat_fname, fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading embedding_matrix(glove): ', dat_fname)\n",
        "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        print('loading word vectors...(glove)')\n",
        "        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
        "        fname = fname + '/glove.twitter.27B/glove.twitter.27B.' + str(embed_dim) + 'd.txt' \\\n",
        "            if embed_dim != 300 else fname + '/glove.840B.300d.txt'\n",
        "        word_vec = _load_word_vec_glove(fname, word2idx=word2idx)\n",
        "        print('building embedding_matrix:', dat_fname)\n",
        "        for word, i in word2idx.items():\n",
        "            vec = word_vec.get(word)\n",
        "            if vec is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = vec\n",
        "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n",
        "    return embedding_matrix\n",
        "\n",
        "def build_embedding_matrix_BioASQ(word2idx, embed_dim, dat_fname, fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading embedding_matrix(BioASQ):', dat_fname)\n",
        "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        print('loading word vectors...(BioASQ)')\n",
        "        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
        "        f = open(fname + \"/word2vecTools/types.txt\",\"r\")\n",
        "        i = 0\n",
        "        names = []\n",
        "        for line in f:\n",
        "            names.append(line.split('\\n')[0])\n",
        "            i = i + 1\n",
        "        vectors = np.loadtxt(fname + \"/word2vecTools/vectors.txt\")\n",
        "        word_vec = {}\n",
        "        for (index, name) in enumerate(names):\n",
        "            word_vec[name] = index\n",
        "\n",
        "        print('building embedding_matrix:', dat_fname)\n",
        "        for word, i in word2idx.items():\n",
        "            if word in word_vec.keys():\n",
        "                vec = vectors[word_vec[word]]\n",
        "            else:\n",
        "                vec = None\n",
        "            if vec is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = vec\n",
        "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n",
        "    return embedding_matrix\n",
        "\n",
        "def convert_examples_to_features(\n",
        "    examples,\n",
        "    tokenizer,\n",
        "    tokenizer_cleantext,\n",
        "    max_length=512,\n",
        "    task=None,\n",
        "    label_list=None,\n",
        "    output_mode=None,\n",
        "    pad_on_left=False,\n",
        "    pad_token=0,\n",
        "    pad_token_segment_id=0,\n",
        "    mask_padding_with_zero=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads a data file into a list of ``ICHI_InputFeatures``\n",
        "\n",
        "    Args:\n",
        "        examples: List of ``ICHI_InputExamples`` or ``tf.data.Dataset`` containing the examples.\n",
        "        tokenizer: Instance of a tokenizer that will tokenize the examples\n",
        "        tokenizer_cleantext: Instance of a tokenizer that will tokenize the examples clean text(used for our medical module) \n",
        "        max_length: Maximum example length\n",
        "        task: GLUE task\n",
        "        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method\n",
        "        output_mode: String indicating the output mode. Either ``regression`` or ``classification``\n",
        "        pad_on_left: If set to ``True``, the examples will be padded on the left rather than on the right (default)\n",
        "        pad_token: Padding token\n",
        "        pad_token_segment_id: The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)\n",
        "        mask_padding_with_zero: If set to ``True``, the attention mask will be filled by ``1`` for actual values\n",
        "            and by ``0`` for padded values. If set to ``False``, inverts it (``1`` for padded values, ``0`` for\n",
        "            actual values)\n",
        "\n",
        "    Returns:\n",
        "        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n",
        "        containing the task-specific features. If the input is a list of ``ICHI_InputExamples``, will return\n",
        "        a list of task-specific ``ICHI_InputFeatures`` which can be fed to the model.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    is_tf_dataset = False\n",
        "    if is_tf_available() and isinstance(examples, tf.data.Dataset):\n",
        "        is_tf_dataset = True\n",
        "\n",
        "    \"\"\"      Initialisation of Data Processor    \"\"\"\n",
        "    if task is not None:\n",
        "        processor = ICHIProcessor()  \n",
        "        if label_list is None:\n",
        "            label_list = processor.get_labels()\n",
        "            logger.info(\"Using label list %s for task %s\" % (label_list, task))\n",
        "        if output_mode is None:\n",
        "            output_mode = \"classification\"\n",
        "            logger.info(\"Using output mode %s for task %s\" % (output_mode, task))\n",
        "\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "    \"\"\"      Processing the examples    \"\"\"\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10000 == 0:\n",
        "            logger.info(\"Writing example %d/%d\" % (ex_index, len(examples)))\n",
        "        if is_tf_dataset:\n",
        "            example = processor.get_example_from_tensor_dict(example)\n",
        "            example = processor.tfds_map(example)\n",
        "        \"\"\"      Assuming that each aspect consist of max 4 tokens hence making sure that aspects total tokens coming from aspects are not greater than max sequence length    \"\"\"\n",
        "        aspect_present = bool(example.aspects is not None)\n",
        "        if aspect_present:\n",
        "            if 4 * len(example.aspects) > max_length:\n",
        "                example.aspects = random.sample(example.aspects, k = int(max_length/4))\n",
        "                \n",
        "        \"\"\"      Using tokenizer.encode_plus_lcf the global features are encoded as [<special token> + text_tokens + <special token> + heading_tokens + <special token> + aspect1_token + <special token> + .... + aspectn_token + <special token>] \"\"\"\n",
        "        inputs_global = tokenizer.encode_plus_lcf(example.text, example.heading, example.aspects, add_special_tokens=True, max_length=max_length,) ######## edittttttt\n",
        "        \"\"\"      Using tokenizer.encode_plus_lcf the local features(medical module) are encoded as [<special token> + text_tokens + <special token>] \"\"\"\n",
        "        inputs_local = tokenizer.encode_plus_lcf(example.text, add_special_tokens=True, max_length=max_length,)\n",
        "        \"\"\" Generating tokens for the clean_text i.e. tokenising text based on glove or BioASQ \"\"\"\n",
        "        text_clean_indices = tokenizer_cleantext.text_to_sequence(example.clean_text)\n",
        "        \n",
        "        \"\"\"      Using tokenizer.encode_plus_lcf the aspect_indices are encoded as [<special token> + aspect1_token + <special token> + .... + aspectn_token + <special token>] \"\"\"\n",
        "        if len(example.aspects) > 2:\n",
        "            aspect_indices = tokenizer.encode_plus_lcf(example.aspects[0], example.aspects[1], example.aspects[2:], add_special_tokens=False, max_length=max_length,)\n",
        "        elif len(example.aspects) == 2:\n",
        "            aspect_indices = tokenizer.encode_plus_lcf(example.aspects[0], example.aspects[1], add_special_tokens=False, max_length=max_length,)\n",
        "        else:\n",
        "            aspect_indices = tokenizer.encode_plus_lcf(example.aspects[0], add_special_tokens=False, max_length=max_length,)\n",
        "        \n",
        "        input_global_ids, token_global_type_ids = inputs_global[\"input_ids\"], inputs_global[\"token_type_ids\"]\n",
        "        input_local_ids, token_local_type_ids = inputs_local[\"input_ids\"], inputs_local[\"token_type_ids\"]\n",
        "        \n",
        "        aspect_indices = aspect_indices[\"input_ids\"]\n",
        "        padding_length_aspect = max_length - len(aspect_indices)\n",
        "        aspect_indices = aspect_indices + ([pad_token] * padding_length_aspect)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        attention_mask_global = [1 if mask_padding_with_zero else 0] * len(input_global_ids)\n",
        "        attention_mask_local = [1 if mask_padding_with_zero else 0] * len(input_local_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length_global = max_length - len(input_global_ids)\n",
        "        padding_length_local = max_length - len(input_local_ids)\n",
        "        if pad_on_left:\n",
        "            input_global_ids = ([pad_token] * padding_length_global) + input_global_ids\n",
        "            attention_mask_global = ([0 if mask_padding_with_zero else 1] * padding_length_global) + attention_mask_global\n",
        "            token_global_type_ids = ([pad_token_segment_id] * padding_length_global) + token_global_type_ids\n",
        "            input_local_ids = ([pad_token] * padding_length_local) + input_local_ids\n",
        "            attention_mask_local = ([0 if mask_padding_with_zero else 1] * padding_length_local) + attention_mask_local\n",
        "            token_local_type_ids = ([pad_token_segment_id] * padding_length_local) + token_local_type_ids\n",
        "        else:\n",
        "            input_global_ids = input_global_ids + ([pad_token] * padding_length_global)\n",
        "            attention_mask_global = attention_mask_global + ([0 if mask_padding_with_zero else 1] * padding_length_global)\n",
        "            token_global_type_ids = token_global_type_ids + ([pad_token_segment_id] * padding_length_global)\n",
        "            input_local_ids = input_local_ids + ([pad_token] * padding_length_local)\n",
        "            attention_mask_local = attention_mask_local + ([0 if mask_padding_with_zero else 1] * padding_length_local)\n",
        "            token_local_type_ids = token_local_type_ids + ([pad_token_segment_id] * padding_length_local)\n",
        "\n",
        "        assert len(input_global_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_global_ids), max_length)\n",
        "        assert len(input_local_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_local_ids), max_length)\n",
        "        assert len(attention_mask_global) == max_length, \"Error with input length {} vs {}\".format(\n",
        "            len(attention_mask_global), max_length\n",
        "        )\n",
        "        assert len(attention_mask_local) == max_length, \"Error with input length {} vs {}\".format(\n",
        "            len(attention_mask_local), max_length\n",
        "        )\n",
        "        assert len(token_global_type_ids) == max_length, \"Error with input length {} vs {}\".format(\n",
        "            len(token_global_type_ids), max_length\n",
        "        )\n",
        "        assert len(token_local_type_ids) == max_length, \"Error with input length {} vs {}\".format(\n",
        "            len(token_local_type_ids), max_length\n",
        "        )\n",
        "\n",
        "        if output_mode == \"classification\":\n",
        "            label = label_map[example.label]\n",
        "        elif output_mode == \"regression\":\n",
        "            label = float(example.label)\n",
        "        else:\n",
        "            raise KeyError(output_mode)\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"input_global_ids: %s\" % \" \".join([str(x) for x in input_global_ids]))\n",
        "            logger.info(\"attention_mask_global: %s\" % \" \".join([str(x) for x in attention_mask_global]))\n",
        "            logger.info(\"token_global_type_ids: %s\" % \" \".join([str(x) for x in token_global_type_ids]))\n",
        "            logger.info(\"input_local_ids: %s\" % \" \".join([str(x) for x in input_local_ids]))\n",
        "            logger.info(\"attention_mask_local: %s\" % \" \".join([str(x) for x in attention_mask_local]))\n",
        "            logger.info(\"token_local_type_ids: %s\" % \" \".join([str(x) for x in token_local_type_ids]))\n",
        "            logger.info(\"text_clean_indices: %s\" % \" \".join([str(x) for x in text_clean_indices]))\n",
        "            logger.info(\"aspect_indices: %s\" % \" \".join([str(x) for x in aspect_indices]))\n",
        "            logger.info(\"label: %s (id = %d)\" % (example.label, label))\n",
        "\n",
        "        features.append(\n",
        "            ICHI_InputFeatures(\n",
        "                input_global_ids=input_global_ids, input_local_ids=input_local_ids, attention_mask_global=attention_mask_global, attention_mask_local=attention_mask_local,\n",
        "                token_global_type_ids=token_global_type_ids, token_local_type_ids=token_local_type_ids, text_clean_indices=text_clean_indices, aspect_indices=aspect_indices, label=label\n",
        "            )\n",
        "        )\n",
        "\n",
        "    if is_tf_available() and is_tf_dataset:\n",
        "\n",
        "        def gen():\n",
        "            for ex in features:\n",
        "                yield (\n",
        "                    {\n",
        "                        \"input_global_ids\": ex.input_global_ids,\n",
        "                        \"input_local_ids\": ex.input_local_ids,\n",
        "                        \"attention_mask_global\": ex.attention_mask_global,\n",
        "                        \"attention_mask_local\": ex.attention_mask_local,\n",
        "                        \"token_global_type_ids\": ex.token_global_type_ids,\n",
        "                        \"token_local_type_ids\": ex.token_local_type_ids,\n",
        "                        \"text_clean_indices\": ex.text_clean_indices,\n",
        "                        \"aspect_indices\": ex.aspect_indices,\n",
        "                    },\n",
        "                    ex.label,\n",
        "                )\n",
        "\n",
        "        return tf.data.Dataset.from_generator(\n",
        "            gen,\n",
        "            ({\"input_global_ids\": tf.int32, \"input_local_ids\": tf.int32, \"attention_mask_global\": tf.int32, \"attention_mask_local\": tf.int32, \"token_global_type_ids\": tf.int32, \"token_local_type_ids\": tf.int32, \"text_clean_indices\": tf.int32, \"aspect_indices\": tf.int32}, tf.int64),\n",
        "            (\n",
        "                {\n",
        "                    \"input_global_ids\": tf.TensorShape([None]),\n",
        "                    \"input_local_ids\": tf.TensorShape([None]),\n",
        "                    \"attention_mask_global\": tf.TensorShape([None]),\n",
        "                    \"attention_mask_local\": tf.TensorShape([None]),\n",
        "                    \"token_global_type_ids\": tf.TensorShape([None]),\n",
        "                    \"token_local_type_ids\": tf.TensorShape([None]),\n",
        "                    \"text_clean_indices\": tf.TensorShape([None]),\n",
        "                    \"aspect_indices\": tf.TensorShape([None]),\n",
        "                },\n",
        "                tf.TensorShape([]),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    return features\n",
        "\n",
        "class ICHIProcessor(object):\n",
        "    \"\"\"Processor for the ICHI data set (GLUE version).\"\"\"\n",
        "\n",
        "    def get_example_from_tensor_dict(self, tensor_dict):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return ICHI_InputExample(tensor_dict['idx'].numpy(),\n",
        "                            tensor_dict['heading'].numpy().decode('utf-8'),\n",
        "                            tensor_dict['text'].numpy().decode('utf-8'),\n",
        "                            tensor_dict['clean_text'].numpy().decode('utf-8'),\n",
        "                            tensor_dict['aspects'].numpy().decode('utf-8'),\n",
        "                            str(tensor_dict['label'].numpy()))\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"temp_train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"final_test_result.tsv\")),\n",
        "            \"dev\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"DEMO\", \"DISE\", \"TRMT\", \"GOAL\", \"PREG\", \"FAML\", \"SOCL\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            label = line[0]\n",
        "            heading = None\n",
        "            text = line[1] + ' ' + line[2]\n",
        "            try:\n",
        "                aspects = [' ' + x for x in line[3].split('|')]\n",
        "                temp_clean_text = \" \".join(line[3].split('|'))\n",
        "            except:\n",
        "                aspects = None\n",
        "                temp_clean_text = \"\"\n",
        "                print(\"Sed\")\n",
        "            \n",
        "            # print(aspects)\n",
        "            \"\"\"    IMPORTANT    IMPORTANT    IMPORTANT    IMPORTANT    IMPORTANT   \"\"\"\n",
        "            clean_text = self.clean_str( text, lemmatizer) ######## for using glove embeddings over sentence\n",
        "            # clean_text = self.clean_str( temp_clean_text, lemmatizer) ##### for using BioASQ embeddings in aspects\n",
        "            examples.append(\n",
        "                ICHI_InputExample(guid=guid, heading=heading, text=text, clean_text=clean_text, aspects=aspects, label=label))\n",
        "        return examples\n",
        "\n",
        "    def tfds_map(self, example):\n",
        "        \"\"\"Some tensorflow_datasets datasets are not formatted the same way the GLUE datasets are.\n",
        "        This method converts examples to the correct format.\"\"\"\n",
        "        if len(self.get_labels()) > 1:\n",
        "            example.label = self.get_labels()[int(example.label)]\n",
        "        return example\n",
        "\n",
        "    def clean_str(self, string1, lemmatizer):\n",
        "        \"\"\"\n",
        "        Tokenization/string cleaning for dataset\n",
        "        Every dataset is lower cased except\n",
        "        \"\"\"\n",
        "        str_stop = \"\"\n",
        "        string1 = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '',string1)\n",
        "        string1 = re.sub(r\"\\\\\", \" \", string1)    \n",
        "        string1 = re.sub(r\"\\'\", \" \", string1)    \n",
        "        string1 = re.sub(r\"\\\"\", \" \", string1)   \n",
        "        string1 = re.sub(r'(\\W)\\1+', r'\\1', string1)\n",
        "        word_list=string1.split(\" \")\n",
        "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
        "        for kj in filtered_words:\n",
        "            new=lemmatizer.lemmatize(str(kj)) \n",
        "            str_stop=str_stop +\" \"+new\n",
        "            str_stop.encode('utf-8')\n",
        "        return str_stop.strip().lower()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))\n",
        "\n",
        "class ICHI_InputExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for simple sequence classification.\n",
        "\n",
        "    Args:\n",
        "        guid: Unique id for the example.\n",
        "        heading: string. The untokenized heading of the sequence\n",
        "        text: string. The untokenized text part of the sequence.\n",
        "        clean_text: string. The untokenized text part of the sequence used for medical module(as tokenization will be different for glove and BioASQ than BERT).\n",
        "        aspects: list of string. The untokenized aspects for the sequence as a list of strings\n",
        "        Only must be specified for sequence pair tasks.\n",
        "        label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, guid, heading, text, clean_text, aspects, label=None):\n",
        "        self.guid = guid\n",
        "        self.heading = heading\n",
        "        self.text = text\n",
        "        self.clean_text = clean_text\n",
        "        self.aspects = aspects\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class ICHI_InputFeatures(object):\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "\n",
        "    Args:\n",
        "        input_ids: Indices of input sequence tokens in the vocabulary.\n",
        "        attention_mask: Mask to avoid performing attention on padding token indices.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            Usually  ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded) tokens.\n",
        "        token_type_ids: Segment token indices to indicate first and second portions of the inputs.\n",
        "        label: Label corresponding to the input\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_global_ids, input_local_ids, attention_mask_global=None, attention_mask_local=None, token_global_type_ids=None, token_local_type_ids=None, text_clean_indices=None, aspect_indices=None, label=None):\n",
        "        self.input_global_ids = input_global_ids\n",
        "        self.attention_mask_global = attention_mask_global\n",
        "        self.token_global_type_ids = token_global_type_ids\n",
        "        self.input_local_ids = input_local_ids\n",
        "        self.attention_mask_local = attention_mask_local\n",
        "        self.token_local_type_ids = token_local_type_ids\n",
        "        self.text_clean_indices = text_clean_indices\n",
        "        self.aspect_indices = aspect_indices\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, task, tokenizer, tokenizer_cleantext, evaluate=False):\n",
        "    if args.local_rank not in [-1, 0] and not evaluate:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "    processor = ICHIProcessor()  \n",
        "    output_mode = \"classification\"\n",
        "    # Load data features from cache or dataset file\n",
        "    cached_features_file = os.path.join(\n",
        "        args.data_dir,\n",
        "        \"cached_{}_{}_{}_{}\".format(\n",
        "            \"dev\" if evaluate else \"train\",\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(args.max_seq_length),\n",
        "            str(task),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    \"\"\" Load saved tokenizer for the clean_text\"\"\"\n",
        "    cached_tokenizer_cleantext_file = os.path.join(\n",
        "        args.data_dir,\n",
        "        \"cachedtokenizer_{}_{}_{}\".format(\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(args.embedding_type),\n",
        "            str(task),\n",
        "        ),\n",
        "    )\n",
        "    \"\"\" if overwrite cache is disabled and saved feature and tokenizer file exists then load from the saved file else process them \"\"\"\n",
        "    if os.path.exists(cached_features_file) and os.path.exists(cached_tokenizer_cleantext_file) and not args.overwrite_cache:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "        print('loading tokenizer from cache:', cached_tokenizer_cleantext_file)\n",
        "        tokenizer_cleantext = pickle.load(open(cached_tokenizer_cleantext_file, 'rb'))\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        label_list = processor.get_labels()\n",
        "        if task in [\"mnli\", \"mnli-mm\"] and args.model_type in [\"roberta\", \"xlmroberta\"]:\n",
        "            # HACK(label indices are swapped in RoBERTa pretrained model)\n",
        "            label_list[1], label_list[2] = label_list[2], label_list[1]\n",
        "        examples = (\n",
        "            processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n",
        "        )\n",
        "        if (not evaluate):\n",
        "            tokenizer_cleantext.fit_on_examples(examples)\n",
        "            tokenizer_cleantext.update_tokenizer()\n",
        "        \"\"\" features are created using this function which is defined above\"\"\"\n",
        "        features = convert_examples_to_features(\n",
        "            examples,\n",
        "            tokenizer,\n",
        "            tokenizer_cleantext,\n",
        "            label_list=label_list,\n",
        "            max_length=args.max_seq_length,\n",
        "            output_mode=output_mode,\n",
        "            pad_on_left=bool(args.model_type in [\"xlnet\"]),  # pad on the left for xlnet\n",
        "            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "            pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n",
        "        )\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            torch.save(features, cached_features_file)\n",
        "            pickle.dump(tokenizer_cleantext, open(cached_tokenizer_cleantext_file, 'wb'))\n",
        "\n",
        "    if args.local_rank == 0 and not evaluate:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_global_ids = torch.tensor([f.input_global_ids for f in features], dtype=torch.long)\n",
        "    all_input_local_ids = torch.tensor([f.input_local_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask_global = torch.tensor([f.attention_mask_global for f in features], dtype=torch.long)\n",
        "    all_attention_mask_local = torch.tensor([f.attention_mask_local for f in features], dtype=torch.long)\n",
        "    all_token_global_type_ids = torch.tensor([f.token_global_type_ids for f in features], dtype=torch.long)\n",
        "    all_token_local_type_ids = torch.tensor([f.token_local_type_ids for f in features], dtype=torch.long)\n",
        "    all_text_clean_indices = torch.tensor([f.text_clean_indices for f in features], dtype=torch.long)\n",
        "    all_aspect_indices = torch.tensor([f.aspect_indices for f in features], dtype=torch.long)\n",
        "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    \n",
        "    dataset = TensorDataset(all_input_global_ids, all_input_local_ids, all_attention_mask_global, all_attention_mask_local, all_token_global_type_ids, all_token_local_type_ids, all_text_clean_indices, all_aspect_indices, all_labels)\n",
        "    \n",
        "    return dataset, tokenizer_cleantext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/transformers/examples/ichi/utils_ichi.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyy-AEpoS8Lx",
        "outputId": "d3fb3e83-0e01-415f-ad46-429987112018"
      },
      "source": [
        "%%writefile /content/transformers/examples/ichi/lcf_ichi.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import numpy as np\n",
        "from transformers.modeling_bert import BertPooler, BertSelfAttention, BertPreTrainedModel, BertModel\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "def compute_average_with_padding(tensor, padding):\n",
        "    \"\"\"\n",
        "    :param tensor: dimension batch_size, seq_length, hidden_size\n",
        "    :param padding: dimension batch_size, seq_length\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    batch_size, seq_length, emb_size = tensor.shape\n",
        "    entry_sizes = torch.sum(padding, axis=1)\n",
        "    return torch.sum(tensor, axis=1) / entry_sizes\n",
        "\n",
        "\n",
        "class BertMaxPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states, mask):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = compute_average_with_padding(hidden_states, mask)\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, config, args):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.args = args\n",
        "        self.SA = BertSelfAttention(config)\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        zero_tensor = torch.tensor(np.zeros((inputs.size(0), 1, 1, self.args.max_seq_length),\n",
        "                                            dtype=np.float32), dtype=torch.float32).to(self.args.device)\n",
        "        SA_out = self.SA(inputs, zero_tensor)\n",
        "        return self.tanh(SA_out[0])\n",
        "\n",
        "\n",
        "class lcf_BERT(BertPreTrainedModel):\n",
        "    def __init__(self, config, args):\n",
        "        super(lcf_BERT, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.args = args\n",
        "        self.config =config\n",
        "        self.bert = BertModel(config)\n",
        "        # self.bert_global_focus = self.bert\n",
        "        # self.bert_local_focus = copy.deepcopy(self.bert_global_focus) if args.use_single_bert else self.bert_global_focus\n",
        "        # self.embedder = nn.Embedding.from_pretrained(torch.from_numpy(args.word2vec).float(), freeze=True, padding_idx=0)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # self.lstm = nn.LSTM(args.emb_size, 100, 1, batch_first=True, bidirectional=True)\n",
        "        # Co = 100\n",
        "        # self.conv13 = nn.Conv2d(1, Co, (3, 2*args.emb_size))\n",
        "        # self.conv14 = nn.Conv2d(1, Co, (4, 2*args.emb_size))\n",
        "        # self.conv15 = nn.Conv2d(1, Co, (5, 2*args.emb_size))\n",
        "        # self.dropout_1 = nn.Dropout(0.4)\n",
        "        # self.fc1 = nn.Linear(3*Co, config.num_labels)\n",
        "        # self.fc = nn.Linear(200, config.num_labels)\n",
        "        self.bert_SA = SelfAttention(config, args) ### change\n",
        "        self.linear_double_cdm_or_cdw = nn.Linear(config.hidden_size * 2,config.hidden_size)\n",
        "        self.linear_triple_lcf_global = nn.Linear(config.hidden_size * 3, config.hidden_size)\n",
        "        self.bert_pooler_org = BertPooler(config)\n",
        "        self.bert_pooler = BertMaxPooler(config) ### change\n",
        "        self.dense = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def conv_and_pool(self, x, conv):\n",
        "        x = F.gelu(conv(x)).squeeze(3) # (n, Co, W)\n",
        "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x\n",
        "\n",
        "    def feature_dynamic_mask(self, text_local_indices, aspect_indices):\n",
        "        texts = text_local_indices\n",
        "        # mask_len = self.args.SRD\n",
        "        masked_text_raw_indices = torch.zeros((text_local_indices.size(0), self.args.max_seq_length, self.config.hidden_size),\n",
        "                                          dtype=torch.float)\n",
        "        \n",
        "        masked_text_raw_indices[:, 0, :] = torch.ones((text_local_indices.size(0), self.config.hidden_size), dtype=torch.float) \n",
        "        zero_tensor = torch.tensor(0).to(self.args.device)\n",
        "        for i in range(aspect_indices.shape[0]):\n",
        "            for j in range(aspect_indices[i].shape[0]):\n",
        "                if aspect_indices[i][j] == zero_tensor:\n",
        "                    break\n",
        "                else:\n",
        "                    indices = (text_local_indices[i] == aspect_indices[i][j]).nonzero()\n",
        "                    for k in indices:\n",
        "                        masked_text_raw_indices[i][k] = torch.ones(self.config.hidden_size, dtype=torch.float)\n",
        "        return masked_text_raw_indices.to(self.args.device)\n",
        "\n",
        "    # # create the weights tensor for local context features\n",
        "    # def feature_dynamic_weighted(self, text_local_indices, aspect_indices):\n",
        "    #     texts = text_local_indices\n",
        "    #     asps = aspect_indices\n",
        "    #     # mask_len = self.args.SRD\n",
        "    #     masked_text_raw_indices = torch.zeros((text_local_indices.size(0), self.args.max_seq_length, self.config.hidden_size),\n",
        "    #                                       dtype=torch.float)\n",
        "    #     for text_i, asp_i in zip(range(len(texts)), range(len(asps))):\n",
        "    #         asp_len = np.count_nonzero(asps[asp_i]) - 2\n",
        "    #         try:\n",
        "    #             asp_begin = np.argwhere(texts[text_i] == asps[asp_i][1])[0][0]\n",
        "    #             asp_avg_index = (asp_begin * 2 + asp_len) / 2\n",
        "    #         except:\n",
        "    #             continue\n",
        "    #         distances = np.zeros(np.count_nonzero(texts[text_i]), dtype=np.float32)\n",
        "    #         for i in range(1, np.count_nonzero(texts[text_i])-1):\n",
        "    #             if abs(i - asp_avg_index) + asp_len / 2 > self.args.SRD:\n",
        "    #                 distances[i] = 1 - (abs(i - asp_avg_index)+asp_len/2\n",
        "    #                                     - self.args.SRD)/np.count_nonzero(texts[text_i])\n",
        "    #             else:\n",
        "    #                 distances[i] = 1\n",
        "    #         for i in range(len(distances)):\n",
        "    #             masked_text_raw_indices[text_i][i] = masked_text_raw_indices[text_i][i] * distances[i]\n",
        "    #     # masked_text_raw_indices = torch.from_numpy(masked_text_raw_indices)\n",
        "    #     return masked_text_raw_indices.to(self.args.device)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_global_ids=None,\n",
        "        attention_mask_global=None,\n",
        "        token_global_type_ids=None,\n",
        "        input_local_ids=None,\n",
        "        attention_mask_local=None,\n",
        "        token_local_type_ids=None,\n",
        "        text_clean_indices=None,\n",
        "        aspect_indices=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "\n",
        "        # word_vectors = self.embedder(text_clean_indices)\n",
        "        # self.lstm.flatten_parameters()\n",
        "        # out, _ = self.lstm(word_vectors)\n",
        "        # # out = out.unsqueeze(1)   #### for using CNN\n",
        "        # # print(out.size())\n",
        "        # # print(out)\n",
        "        # logits = self.fc(out[:, -1, :])\n",
        "        # # print(logits.size())\n",
        "        # # print(logits)\n",
        "        # # raise ValueError\n",
        "        # # logits = self.fc(torch.div(word_vectors.sum(axis=1), word_vectors.shape[1]))\n",
        "\n",
        "        global_outputs, _ = self.bert(\n",
        "            input_global_ids,\n",
        "            attention_mask=attention_mask_global,\n",
        "            token_type_ids=token_global_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        local_outputs, _ = self.bert(\n",
        "            input_local_ids,\n",
        "            attention_mask=attention_mask_local,\n",
        "            token_type_ids=token_local_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        # out_cat = torch.cat((local_outputs, global_outputs), dim=-1)\n",
        "        # out_cat = self.linear_double_cdm_or_cdw(out_cat)\n",
        "        \n",
        "#        if self.args.local_context_focus == 'cdm':\n",
        "#            masked_local_text_vec = self.feature_dynamic_mask(input_local_ids, aspect_indices)\n",
        "#            local_outputs = torch.mul(local_outputs, masked_local_text_vec)\n",
        "            # out_cat = torch.cat((local_outputs, global_outputs), dim=-1)\n",
        "            # out_cat = self.linear_double_cdm_or_cdw(out_cat)\n",
        "\n",
        "#        elif self.args.local_context_focus == 'cdw':\n",
        "#            weighted_text_local_features = self.feature_dynamic_weighted(input_local_ids, aspect_indices)\n",
        "#            local_outputs = torch.mul(local_outputs, weighted_text_local_features)\n",
        "#            out_cat = torch.cat((local_outputs, global_outputs), dim=-1)\n",
        "#            out_cat = self.linear_double_cdm_or_cdw(out_cat)\n",
        "\n",
        "#        elif self.args.local_context_focus == 'lcf_fusion':\n",
        "#            masked_local_text_vec = self.feature_dynamic_mask(text_local_indices, aspect_indices)\n",
        "#            masked_local_out = torch.mul(local_outputs, masked_local_text_vec)\n",
        "#            weighted_text_local_features = self.feature_dynamic_weighted(text_local_indices, aspect_indices)\n",
        "#            weighted_local_out = torch.mul(local_outputs, weighted_text_local_features)\n",
        "#            out_cat = torch.cat((masked_local_out, global_outputs, weighted_local_out), dim=-1)\n",
        "#            out_cat = self.linear_triple_lcf_global(out_cat)\n",
        "\n",
        "        # self_attention_out = self.bert_SA(local_outputs)\n",
        "#        self_attention_out = self.dropout(local_outputs)\n",
        "#        local_pooled_out = self.bert_pooler(self_attention_out, masked_local_text_vec)\n",
        "#        self_attention_out = self.dropout(global_outputs)\n",
        "#        global_pooled_out = self.bert_pooler_org(self_attention_out)\n",
        "#        pooled_out = torch.cat((local_pooled_out, global_pooled_out), dim=-1)\n",
        "#        logits = self.dense(pooled_out)\n",
        "#        outputs = (logits,)\n",
        "        self_attention_out = self.dropout(global_outputs)\n",
        "        global_pooled_out = self.bert_pooler_org(self_attention_out) \n",
        "        logits= self.dense(global_pooled_out)\n",
        "        outputs = (logits,)\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/transformers/examples/ichi/lcf_ichi.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWhlcZgHS8a9",
        "outputId": "b47862da-883c-4189-d3bb-eb483591a31b"
      },
      "source": [
        "%%writefile /content/transformers/examples/ichi/run_ichi.py\n",
        "import argparse\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import (\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AlbertConfig,\n",
        "    AlbertForSequenceClassification,\n",
        "    AlbertTokenizer,\n",
        "    BertConfig,\n",
        "    BertForSequenceClassification,\n",
        "    BertTokenizer,\n",
        "    DistilBertConfig,\n",
        "    DistilBertForSequenceClassification,\n",
        "    DistilBertTokenizer,\n",
        "    RobertaConfig,\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizer,\n",
        "    XLMConfig,\n",
        "    XLMForSequenceClassification,\n",
        "    XLMRobertaConfig,\n",
        "    XLMRobertaForSequenceClassification,\n",
        "    XLMRobertaTokenizer,\n",
        "    XLMTokenizer,\n",
        "    XLNetConfig,\n",
        "    XLNetForSequenceClassification,\n",
        "    XLNetTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from lcf_ichi import lcf_BERT#, lcf_XLNET, lcf_XLM, lcf_Roberta, lcf_DistilBert, lcf_Albert, lcf_XLMRoberta\n",
        "from utils_ichi import convert_examples_to_features, ICHIProcessor, compute_metrics, load_and_cache_examples, Tokenizer, pad_and_truncate, build_embedding_matrix_glove, build_embedding_matrix_BioASQ\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except ImportError:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "ALL_MODELS = sum(\n",
        "    (\n",
        "        tuple(conf.pretrained_config_archive_map.keys())\n",
        "        for conf in (\n",
        "            BertConfig,\n",
        "            XLNetConfig,\n",
        "            XLMConfig,\n",
        "            RobertaConfig,\n",
        "            DistilBertConfig,\n",
        "            AlbertConfig,\n",
        "            XLMRobertaConfig,\n",
        "        )\n",
        "    ),\n",
        "    (),\n",
        ")\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    \"bert\": (BertConfig, BertForSequenceClassification, BertTokenizer, lcf_BERT),\n",
        "}\n",
        "#     \"xlnet\": (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer, lcf_XLNET),\n",
        "#     \"xlm\": (XLMConfig, XLMForSequenceClassification, XLMTokenizer, lcf_XLM),\n",
        "#     \"roberta\": (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer, lcf_Roberta),\n",
        "#     \"distilbert\": (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer, lcf_DistilBert),\n",
        "#     \"albert\": (AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer, lcf_Albert),\n",
        "#     \"xlmroberta\": (XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer, lcf_XLMRoberta),\n",
        "# }\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def train(args, train_dataset, model, tokenizer, tokenizer_cleantext):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
        "        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True,\n",
        "        )\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps\n",
        "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
        "    )\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if os.path.exists(args.model_name_or_path):\n",
        "        # set global_step to gobal_step of last saved checkpoint from model path\n",
        "        global_step = int(args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n",
        "        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "\n",
        "        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "        logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "        logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "        logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0],\n",
        "    )\n",
        "    set_seed(args)  # Added here for reproductibility\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "            inputs = {\"input_global_ids\": batch[0], \"attention_mask_global\": batch[2], \"input_local_ids\": batch[1], \"attention_mask_local\": batch[3], \"text_clean_indices\": batch[6], \"aspect_indices\": batch[7], \"labels\": batch[8]}\n",
        "            if args.model_type != \"distilbert\":\n",
        "                inputs[\"token_global_type_ids\"] = (\n",
        "                    batch[4] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
        "                )\n",
        "                inputs[\"token_local_type_ids\"] = (\n",
        "                    batch[5] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
        "                )\n",
        "            # inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
        "            # if args.model_type != \"distilbert\":\n",
        "            #     inputs[\"token_type_ids\"] = (\n",
        "            #         batch[2] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
        "            #     )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    logs = {}\n",
        "                    if (\n",
        "                        args.local_rank == -1 and args.evaluate_during_training\n",
        "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer, tokenizer_cleantext)\n",
        "                        for key, value in results.items():\n",
        "                            eval_key = \"eval_{}\".format(key)\n",
        "                            logs[eval_key] = value\n",
        "\n",
        "                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps\n",
        "                    learning_rate_scalar = scheduler.get_lr()[0]\n",
        "                    logs[\"learning_rate\"] = learning_rate_scalar\n",
        "                    logs[\"loss\"] = loss_scalar\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                    for key, value in logs.items():\n",
        "                        tb_writer.add_scalar(key, value, global_step)\n",
        "                    print(json.dumps({**logs, **{\"step\": global_step}}))\n",
        "\n",
        "                # if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                #     # Save model checkpoint\n",
        "                #     output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "                #     if not os.path.exists(output_dir):\n",
        "                #         os.makedirs(output_dir)\n",
        "                #     # model_to_save = (\n",
        "                #     #     model.module if hasattr(model, \"module\") else model\n",
        "                #     # )  # Take care of distributed/parallel training\n",
        "                #     # model_to_save.save_pretrained(output_dir)\n",
        "                #     torch.save(model.state_dict(), args.output_dir)\n",
        "                #     tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                #     torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                #     logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                #     torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                #     torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                #     logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer, tokenizer_cleantext, prefix=\"\"):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_task_names = (\"ichi\",)\n",
        "    eval_outputs_dirs = (args.output_dir,)\n",
        "\n",
        "    results = {}\n",
        "    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n",
        "        eval_dataset, tokenizer_cleantext = load_and_cache_examples(args, eval_task, tokenizer, tokenizer_cleantext, evaluate=True)\n",
        "\n",
        "        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "            os.makedirs(eval_output_dir)\n",
        "\n",
        "        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "        # Note that DistributedSampler samples randomly\n",
        "        eval_sampler = SequentialSampler(eval_dataset)\n",
        "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "        # multi-gpu eval\n",
        "        if args.n_gpu > 1:\n",
        "            model = torch.nn.DataParallel(model)\n",
        "\n",
        "        # Eval!\n",
        "        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "        eval_loss = 0.0\n",
        "        nb_eval_steps = 0\n",
        "        preds = None\n",
        "        preds_original = None\n",
        "        out_label_ids = None\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            model.eval()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                inputs = {\"input_global_ids\": batch[0], \"attention_mask_global\": batch[2], \"input_local_ids\": batch[1], \"attention_mask_local\": batch[3], \"text_clean_indices\": batch[6], \"aspect_indices\": batch[7], \"labels\": batch[8]}\n",
        "                if args.model_type != \"distilbert\":\n",
        "                    inputs[\"token_global_type_ids\"] = (\n",
        "                        batch[4] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
        "                    )\n",
        "                    inputs[\"token_local_type_ids\"] = (\n",
        "                        batch[5] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
        "                    )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n",
        "                outputs = model(**inputs)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "                eval_loss += tmp_eval_loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "            if preds is None:\n",
        "                preds = logits.detach().cpu().numpy()\n",
        "                preds_original = logits.detach().cpu().numpy()\n",
        "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "                preds_original = np.append(preds_original, logits.detach().cpu().numpy(), axis=0)\n",
        "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "        result = compute_metrics(preds, out_label_ids) ######################update kar !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        results.update(result)\n",
        "\n",
        "        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "            for key in sorted(result.keys()):\n",
        "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "            # for a in preds:\n",
        "            #     writer.write(str(a)+'\\n')\n",
        "            writer.write('[')\n",
        "            for a in preds_original:\n",
        "                writer.write('[')\n",
        "                for c in range(len(a)):\n",
        "                    b = a[c]\n",
        "                    if c!=6:\n",
        "                        writer.write(str(b)+',')\n",
        "                    else:\n",
        "                        writer.write(str(b))\n",
        "                writer.write(']')\n",
        "                writer.write('\\n')\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Required parameters\n",
        "    parser.add_argument(\n",
        "        \"--data_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_type\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name_or_path\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
        "    )\n",
        "\n",
        "    # Other parameters\n",
        "    parser.add_argument(\n",
        "        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--tokenizer_name\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--cache_dir\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "        help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_seq_length\",\n",
        "        default=128,\n",
        "        type=int,\n",
        "        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "        \"than this will be truncated, sequences shorter will be padded.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--embedding_dim_word2vec\",\n",
        "        default=300,\n",
        "        type=int,\n",
        "        help=\"embedding dimension for the word vectors in the medical module\",\n",
        "    )\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\n",
        "        \"--evaluate_during_training\", action=\"store_true\", help=\"Rul evaluation during training at each logging step.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\n",
        "        \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_steps\",\n",
        "        default=-1,\n",
        "        type=int,\n",
        "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
        "    )\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=50, help=\"Log every X updates steps.\")\n",
        "    parser.add_argument(\"--save_steps\", type=int, default=10000, help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument(\n",
        "        \"--eval_all_checkpoints\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
        "    )\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fp16_opt_level\",\n",
        "        type=str,\n",
        "        default=\"O1\",\n",
        "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
        "    )\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if (\n",
        "        os.path.exists(args.output_dir)\n",
        "        and os.listdir(args.output_dir)\n",
        "        and args.do_train\n",
        "        and not args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
        "                args.output_dir\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend=\"nccl\")\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        args.local_rank,\n",
        "        device,\n",
        "        args.n_gpu,\n",
        "        bool(args.local_rank != -1),\n",
        "        args.fp16,\n",
        "    )\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args)\n",
        "\n",
        "    # Prepare GLUE task\n",
        "    args.task_name = 'ichi'\n",
        "    processor = ICHIProcessor()\n",
        "    args.output_mode = \"classification\"\n",
        "    label_list = processor.get_labels()\n",
        "    num_labels = len(label_list)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
        "\n",
        "    args.model_type = args.model_type.lower()\n",
        "    config_class, model_class, tokenizer_class, lcf_model = MODEL_CLASSES[args.model_type]\n",
        "    config = config_class.from_pretrained(\n",
        "        args.config_name if args.config_name else args.model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        finetuning_task=args.task_name,\n",
        "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "    )\n",
        "    tokenizer = tokenizer_class.from_pretrained(\n",
        "        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
        "        do_lower_case=args.do_lower_case,\n",
        "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "    )\n",
        "    tokenizer_cleantext = Tokenizer(max_seq_len = 1600, max_num_words=20000,lower=True)\n",
        "    # model = model_class.from_pretrained(\n",
        "    #     args.model_name_or_path,\n",
        "    #     from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "    #     config=config,\n",
        "    #     cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "    #     force_download=True,\n",
        "    # )\n",
        "\n",
        "    args.use_single_bert = False\n",
        "    args.local_context_focus = 'cdm'\n",
        "    args.embedding_type = 'glove'\n",
        "\n",
        "    cached_embeddingmatrix_file = os.path.join(\n",
        "        args.data_dir,\n",
        "        \"cachedword2vec_{}_{}\".format(\n",
        "            str(args.task_name),\n",
        "            str(\"glove\"),\n",
        "        ),\n",
        "    )\n",
        "    cached_embeddingmatrix_path = \".\"\n",
        "    \n",
        "    train_dataset, tokenizer_cleantext = load_and_cache_examples(args, args.task_name, tokenizer, tokenizer_cleantext, evaluate=False)\n",
        "    # embedding_matrix = build_embedding_matrix_glove(tokenizer_cleantext.word2idx, args.embedding_dim_word2vec, cached_embeddingmatrix_file, cached_embeddingmatrix_path)\n",
        "    # print(embedding_matrix.shape)\n",
        "    # print(tokenizer_cleantext.word2idx)\n",
        "    \n",
        "    # args.word2vec = embedding_matrix\n",
        "    # args.emb_size = embedding_matrix.shape[1]\n",
        "    model = lcf_model.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "        config=config,\n",
        "        args=args,\n",
        "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "    )\n",
        "    # model = lcf_Roberta.from_pretrained(\n",
        "    #     \"/home/bt1/17CS10037/new_transformers/transformers/roberta-large-pytorch_model.bin\",\n",
        "    #     from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "    #     config=config,\n",
        "    #     args=args,\n",
        "    #     cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "    #     force_download=True,\n",
        "    # )\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
        "\n",
        "    model.to(args.device)\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "    \n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        # train_dataset, tokenizer_cleantext = load_and_cache_examples(args, args.task_name, tokenizer, tokenizer_cleantext, evaluate=False)\n",
        "        # matrix = build_embedding_matrix_glove(tokenizer_cleantext.word2idx, args.embedding_dim_word2vec, cached_glove_embeddingmatrix_file, cached_glove_embeddingmatrix_path)\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer, tokenizer_cleantext)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    result = evaluate(args, model, tokenizer, tokenizer_cleantext)\n",
        "    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
        "            os.makedirs(args.output_dir)\n",
        "\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        # model_to_save = (\n",
        "        #     model.module if hasattr(model, \"module\") else model\n",
        "        # )  # Take care of distributed/parallel training\n",
        "        # model_to_save.save_pretrained(args.output_dir)\n",
        "        model_name = \"{}.pt\".format(args.model_type)\n",
        "        torch.save(model.state_dict(), os.path.join(args.output_dir,model_name))\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "        # Load a trained model and vocabulary that you have fine-tuned\n",
        "        # model = model_class.from_pretrained(args.output_dir)\n",
        "        model = lcf_model(config, args)\n",
        "        model.load_state_dict(torch.load(os.path.join(args.output_dir,model_name)))\n",
        "        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n",
        "        model.to(args.device)\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
        "        cached_tokenizer_cleantext_file = os.path.join(\n",
        "            args.data_dir,\n",
        "            \"cachedtokenizer_{}_{}_{}\".format(\n",
        "                list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "                str(args.embedding_type),\n",
        "                str(args.task_name),\n",
        "            ),\n",
        "        )\n",
        "        if os.path.exists(cached_tokenizer_cleantext_file):\n",
        "            print('loading tokenizer from cache:', cached_tokenizer_cleantext_file)\n",
        "            tokenizer_cleantext = pickle.load(open(cached_tokenizer_cleantext_file, 'rb'))\n",
        "        checkpoints = [args.output_dir]\n",
        "        if args.eval_all_checkpoints:\n",
        "            checkpoints = list(\n",
        "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
        "            )\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
        "\n",
        "            # model = model_class.from_pretrained(checkpoint)\n",
        "            # model = lcf_model.from_pretrained(\n",
        "            #     args.model_name_or_path,\n",
        "            #     from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "            #     config=config,\n",
        "            #     args=args,\n",
        "            #     cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "            # )\n",
        "\n",
        "            model = lcf_model(config, args)\n",
        "            model_name = \"{}.pt\".format(args.model_type)\n",
        "            model.load_state_dict(torch.load(os.path.join(args.output_dir,model_name)))\n",
        "            model.to(args.device)\n",
        "            result = evaluate(args, model, tokenizer, tokenizer_cleantext, prefix=prefix)\n",
        "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/transformers/examples/ichi/run_ichi.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmpFdc6sbALY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd5bede-7099-4a24-e7b2-96fdb25f34e8"
      },
      "source": [
        "!python ./examples/ichi/run_ichi.py --model_type bert --model_name_or_path bert-base-uncased --do_lower_case --do_train --do_eval --data_dir ./ichi --max_seq_length 256 --per_gpu_eval_batch_size=16 --per_gpu_train_batch_size=16 --learning_rate 1e-4 --num_train_epochs 2 --output_dir ./tmp/ichi_bert_base_new --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-12 10:41:09.995044: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/12/2021 10:41:11 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "05/12/2021 10:41:12 - INFO - filelock -   Lock 140115777713040 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "05/12/2021 10:41:12 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpcz2caltv\n",
            "Downloading: 100% 433/433 [00:00<00:00, 417kB/s]\n",
            "05/12/2021 10:41:12 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "05/12/2021 10:41:12 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "05/12/2021 10:41:12 - INFO - filelock -   Lock 140115777713040 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "05/12/2021 10:41:12 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "05/12/2021 10:41:12 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": \"ichi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 7,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/12/2021 10:41:12 - INFO - filelock -   Lock 140115777806032 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "05/12/2021 10:41:12 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmplev275fw\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 897kB/s]\n",
            "05/12/2021 10:41:13 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "05/12/2021 10:41:13 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "05/12/2021 10:41:13 - INFO - filelock -   Lock 140115777806032 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "05/12/2021 10:41:13 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "05/12/2021 10:41:13 - INFO - utils_ichi -   Creating features from dataset file at ./ichi\n",
            "05/12/2021 10:42:05 - INFO - utils_ichi -   Writing example 0/3999\n",
            "Traceback (most recent call last):\n",
            "  File \"./examples/ichi/run_ichi.py\", line 704, in <module>\n",
            "    main()\n",
            "  File \"./examples/ichi/run_ichi.py\", line 592, in main\n",
            "    train_dataset, tokenizer_cleantext = load_and_cache_examples(args, args.task_name, tokenizer, tokenizer_cleantext, evaluate=False)\n",
            "  File \"/content/transformers/examples/ichi/utils_ichi.py\", line 600, in load_and_cache_examples\n",
            "    pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n",
            "  File \"/content/transformers/examples/ichi/utils_ichi.py\", line 263, in convert_examples_to_features\n",
            "    inputs_global = tokenizer.encode_plus_lcf(example.text, example.heading, example.aspects, add_special_tokens=True, max_length=max_length,) ######## edittttttt\n",
            "AttributeError: 'BertTokenizer' object has no attribute 'encode_plus_lcf'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H01TYmyzS8n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10681028-8523-437c-df9e-87fa2f17d255"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr 28 11:12:40 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0    40W /  70W |      0MiB / 15109MiB |     23%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PlJzq0GsRjF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g11ukVnhuyD2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9QFTiYbc0nA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}